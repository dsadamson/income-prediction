{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from pyspark.sql import Row\n",
    "# Import struct fields that we can use\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataCleaning\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "file = \"income.csv\"\n",
    "spark.sparkContext.addFile(file)\n",
    "df = spark.read.csv(SparkFiles.get(\"income.csv\"), sep=\",\", header=True)\n",
    "df.createOrReplaceTempView('income_spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+------+------------+-------------+------------------+-----------------+--------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "|age|       workclass|fnlwgt|   education|education_num|    marital_status|       occupation|  relationship| race|   sex|capital_gain|capital_loss|hours_per_week|native_country|income|\n",
      "+---+----------------+------+------------+-------------+------------------+-----------------+--------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "| 40|Self-emp-not-inc|223881| Prof-school|           15|Married-civ-spouse|   Prof-specialty|       Husband|White|  Male|       99999|           0|            70| United-States|  >50K|\n",
      "| 30|         Private|149118|     HS-grad|            9|          Divorced|     Craft-repair| Not-in-family|White|Female|           0|           0|            40| United-States| <=50K|\n",
      "| 46|         Private|109209|Some-college|           10|Married-civ-spouse|     Adm-clerical|       Husband|White|  Male|           0|           0|            40| United-States|  >50K|\n",
      "| 32|         Private|229566|   Assoc-voc|           11|Married-civ-spouse|    Other-service|       Husband|White|  Male|           0|           0|            60| United-States|  >50K|\n",
      "| 54|               ?|148657|   Preschool|            1|Married-civ-spouse|                ?|          Wife|White|Female|           0|           0|            40|        Mexico| <=50K|\n",
      "| 63|         Private|111963|Some-college|           10|Married-civ-spouse|   Prof-specialty|       Husband|White|  Male|           0|           0|            16| United-States| <=50K|\n",
      "| 25|         Private|207875|     7th-8th|            4|Married-civ-spouse|Handlers-cleaners|       Husband|White|  Male|           0|           0|            40|        Mexico| <=50K|\n",
      "| 71|       Local-gov|229110|     HS-grad|            9|           Widowed|  Exec-managerial|Other-relative|White|Female|           0|           0|            33| United-States| <=50K|\n",
      "| 37|         Private| 66686|     HS-grad|            9|Married-civ-spouse|Handlers-cleaners|       Husband|White|  Male|           0|           0|            40| United-States| <=50K|\n",
      "| 44|         Private|227399|   Assoc-voc|           11|Married-civ-spouse|  Exec-managerial|       Husband|Black|  Male|           0|           0|            40| United-States| <=50K|\n",
      "| 21|         Private| 57916|     HS-grad|            9|     Never-married|  Protective-serv|     Own-child|White|  Male|           0|           0|            40| United-States| <=50K|\n",
      "| 37|       Local-gov|244803|     HS-grad|            9|Married-civ-spouse|     Craft-repair|       Husband|White|  Male|           0|           0|            40|             ?| <=50K|\n",
      "| 18|         Private|127388|        12th|            8|     Never-married|    Other-service| Not-in-family|White|Female|           0|           0|            25| United-States| <=50K|\n",
      "| 33|         Private|117963|   Bachelors|           13|Married-civ-spouse|  Exec-managerial|       Husband|White|  Male|           0|           0|            40| United-States|  >50K|\n",
      "| 29|         Private| 29732|   Bachelors|           13|Married-civ-spouse|   Prof-specialty|          Wife|White|Female|           0|           0|            24| United-States| <=50K|\n",
      "| 46|         Private|369538|Some-college|           10|Married-civ-spouse|            Sales|       Husband|White|  Male|           0|           0|            40| United-States|  >50K|\n",
      "| 58|    Self-emp-inc|174864|     HS-grad|            9|Married-civ-spouse|            Sales|       Husband|White|  Male|           0|           0|            55| United-States|  >50K|\n",
      "| 42|       State-gov|212027|   Bachelors|           13|          Divorced|   Prof-specialty| Not-in-family|Black|  Male|           0|           0|            40| United-States| <=50K|\n",
      "| 45|       State-gov|110311|     Masters|           14|          Divorced|  Exec-managerial|     Unmarried|White|Female|           0|           0|            40| United-States| <=50K|\n",
      "| 62|Self-emp-not-inc|102631|Some-college|           10|           Widowed|  Farming-fishing|     Unmarried|White|Female|           0|           0|            50| United-States| <=50K|\n",
      "+---+----------------+------+------------+-------------+------------------+-----------------+--------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#using spark.sql pull all the columsn from the df except for capital.gains, capital.loss, and fnlwgt\n",
    "df = spark.sql(\"\"\"SELECT age, \n",
    "               workclass, \n",
    "               education, \n",
    "               education_num, \n",
    "               marital_status, \n",
    "               occupation, \n",
    "               relationship, \n",
    "               race, \n",
    "               sex, \n",
    "               hours_per_week, \n",
    "               native_country, \n",
    "               income\n",
    "               FROM income_spark\"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the spark dataframe df a pandas dataframe\n",
    "income = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_cat = income.dtypes[income.dtypes == 'object'].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               73\n",
       "workclass          9\n",
       "education         16\n",
       "education_num     16\n",
       "marital_status     7\n",
       "occupation        15\n",
       "relationship       6\n",
       "race               5\n",
       "sex                2\n",
       "hours_per_week    93\n",
       "native_country    41\n",
       "income             2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the number of unique values in each column\n",
    "income[income_cat].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_17</th>\n",
       "      <th>age_18</th>\n",
       "      <th>age_19</th>\n",
       "      <th>age_20</th>\n",
       "      <th>age_21</th>\n",
       "      <th>age_22</th>\n",
       "      <th>age_23</th>\n",
       "      <th>age_24</th>\n",
       "      <th>age_25</th>\n",
       "      <th>age_26</th>\n",
       "      <th>...</th>\n",
       "      <th>native_country_Scotland</th>\n",
       "      <th>native_country_South</th>\n",
       "      <th>native_country_Taiwan</th>\n",
       "      <th>native_country_Thailand</th>\n",
       "      <th>native_country_Trinadad&amp;Tobago</th>\n",
       "      <th>native_country_United-States</th>\n",
       "      <th>native_country_Vietnam</th>\n",
       "      <th>native_country_Yugoslavia</th>\n",
       "      <th>income_&lt;=50K</th>\n",
       "      <th>income_&gt;50K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age_17  age_18  age_19  age_20  age_21  age_22  age_23  age_24  age_25  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   age_26  ...  native_country_Scotland  native_country_South  \\\n",
       "0     0.0  ...                      0.0                   0.0   \n",
       "1     0.0  ...                      0.0                   0.0   \n",
       "2     0.0  ...                      0.0                   0.0   \n",
       "3     0.0  ...                      0.0                   0.0   \n",
       "4     0.0  ...                      0.0                   0.0   \n",
       "\n",
       "   native_country_Taiwan  native_country_Thailand  \\\n",
       "0                    0.0                      0.0   \n",
       "1                    0.0                      0.0   \n",
       "2                    0.0                      0.0   \n",
       "3                    0.0                      0.0   \n",
       "4                    0.0                      0.0   \n",
       "\n",
       "   native_country_Trinadad&Tobago  native_country_United-States  \\\n",
       "0                             0.0                           1.0   \n",
       "1                             0.0                           1.0   \n",
       "2                             0.0                           1.0   \n",
       "3                             0.0                           1.0   \n",
       "4                             0.0                           0.0   \n",
       "\n",
       "   native_country_Vietnam  native_country_Yugoslavia  income_<=50K  \\\n",
       "0                     0.0                        0.0           0.0   \n",
       "1                     0.0                        0.0           1.0   \n",
       "2                     0.0                        0.0           0.0   \n",
       "3                     0.0                        0.0           0.0   \n",
       "4                     0.0                        0.0           1.0   \n",
       "\n",
       "   income_>50K  \n",
       "0          1.0  \n",
       "1          0.0  \n",
       "2          1.0  \n",
       "3          1.0  \n",
       "4          0.0  \n",
       "\n",
       "[5 rows x 285 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(income[income_cat]))\n",
    "\n",
    "# Add the encoded variable names to the dataframe\n",
    "encode_df.columns = enc.get_feature_names_out(income_cat)\n",
    "encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoff\\AppData\\Local\\Temp\\ipykernel_28780\\2929446180.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  income = income.drop(income_cat,1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_17</th>\n",
       "      <th>age_18</th>\n",
       "      <th>age_19</th>\n",
       "      <th>age_20</th>\n",
       "      <th>age_21</th>\n",
       "      <th>age_22</th>\n",
       "      <th>age_23</th>\n",
       "      <th>age_24</th>\n",
       "      <th>age_25</th>\n",
       "      <th>age_26</th>\n",
       "      <th>...</th>\n",
       "      <th>native_country_Scotland</th>\n",
       "      <th>native_country_South</th>\n",
       "      <th>native_country_Taiwan</th>\n",
       "      <th>native_country_Thailand</th>\n",
       "      <th>native_country_Trinadad&amp;Tobago</th>\n",
       "      <th>native_country_United-States</th>\n",
       "      <th>native_country_Vietnam</th>\n",
       "      <th>native_country_Yugoslavia</th>\n",
       "      <th>income_&lt;=50K</th>\n",
       "      <th>income_&gt;50K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age_17  age_18  age_19  age_20  age_21  age_22  age_23  age_24  age_25  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   age_26  ...  native_country_Scotland  native_country_South  \\\n",
       "0     0.0  ...                      0.0                   0.0   \n",
       "1     0.0  ...                      0.0                   0.0   \n",
       "2     0.0  ...                      0.0                   0.0   \n",
       "3     0.0  ...                      0.0                   0.0   \n",
       "4     0.0  ...                      0.0                   0.0   \n",
       "\n",
       "   native_country_Taiwan  native_country_Thailand  \\\n",
       "0                    0.0                      0.0   \n",
       "1                    0.0                      0.0   \n",
       "2                    0.0                      0.0   \n",
       "3                    0.0                      0.0   \n",
       "4                    0.0                      0.0   \n",
       "\n",
       "   native_country_Trinadad&Tobago  native_country_United-States  \\\n",
       "0                             0.0                           1.0   \n",
       "1                             0.0                           1.0   \n",
       "2                             0.0                           1.0   \n",
       "3                             0.0                           1.0   \n",
       "4                             0.0                           0.0   \n",
       "\n",
       "   native_country_Vietnam  native_country_Yugoslavia  income_<=50K  \\\n",
       "0                     0.0                        0.0           0.0   \n",
       "1                     0.0                        0.0           1.0   \n",
       "2                     0.0                        0.0           0.0   \n",
       "3                     0.0                        0.0           0.0   \n",
       "4                     0.0                        0.0           1.0   \n",
       "\n",
       "   income_>50K  \n",
       "0          1.0  \n",
       "1          0.0  \n",
       "2          1.0  \n",
       "3          1.0  \n",
       "4          0.0  \n",
       "\n",
       "[5 rows x 285 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge one-hot encoded features and drop the originals\n",
    "income = income.merge(encode_df,left_index=True, right_index=True)\n",
    "income = income.drop(income_cat,1)\n",
    "income.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop capital.gain, capital.loss, fnlwgt, income_>50K\n",
    "income.drop(['income_>50K'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = income[\"income_<=50K\"].values\n",
    "X = income.drop([\"income_<=50K\"], axis=1).values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 8)                 2272      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 45        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2323 (9.07 KB)\n",
      "Trainable params: 2323 (9.07 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 =  8\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "586/586 [==============================] - 1s 749us/step - loss: 0.4384 - accuracy: 0.7825\n",
      "Epoch 2/100\n",
      "586/586 [==============================] - 0s 736us/step - loss: 0.3546 - accuracy: 0.8326\n",
      "Epoch 3/100\n",
      "586/586 [==============================] - 0s 720us/step - loss: 0.3401 - accuracy: 0.8415\n",
      "Epoch 4/100\n",
      "586/586 [==============================] - 0s 721us/step - loss: 0.3332 - accuracy: 0.8441\n",
      "Epoch 5/100\n",
      "586/586 [==============================] - 0s 727us/step - loss: 0.3285 - accuracy: 0.8467\n",
      "Epoch 6/100\n",
      "586/586 [==============================] - 0s 751us/step - loss: 0.3243 - accuracy: 0.8483\n",
      "Epoch 7/100\n",
      "586/586 [==============================] - 0s 733us/step - loss: 0.3210 - accuracy: 0.8492\n",
      "Epoch 8/100\n",
      "586/586 [==============================] - 0s 735us/step - loss: 0.3186 - accuracy: 0.8492\n",
      "Epoch 9/100\n",
      "586/586 [==============================] - 0s 774us/step - loss: 0.3159 - accuracy: 0.8487\n",
      "Epoch 10/100\n",
      "586/586 [==============================] - 0s 738us/step - loss: 0.3141 - accuracy: 0.8522\n",
      "Epoch 11/100\n",
      "586/586 [==============================] - 0s 733us/step - loss: 0.3120 - accuracy: 0.8528\n",
      "Epoch 12/100\n",
      "586/586 [==============================] - 0s 743us/step - loss: 0.3100 - accuracy: 0.8532\n",
      "Epoch 13/100\n",
      "586/586 [==============================] - 0s 730us/step - loss: 0.3083 - accuracy: 0.8545\n",
      "Epoch 14/100\n",
      "586/586 [==============================] - 0s 761us/step - loss: 0.3074 - accuracy: 0.8547\n",
      "Epoch 15/100\n",
      "586/586 [==============================] - 0s 754us/step - loss: 0.3048 - accuracy: 0.8547\n",
      "Epoch 16/100\n",
      "586/586 [==============================] - 0s 728us/step - loss: 0.3041 - accuracy: 0.8556\n",
      "Epoch 17/100\n",
      "586/586 [==============================] - 0s 734us/step - loss: 0.3028 - accuracy: 0.8553\n",
      "Epoch 18/100\n",
      "586/586 [==============================] - 0s 747us/step - loss: 0.3014 - accuracy: 0.8561\n",
      "Epoch 19/100\n",
      "586/586 [==============================] - 0s 728us/step - loss: 0.3005 - accuracy: 0.8581\n",
      "Epoch 20/100\n",
      "586/586 [==============================] - 0s 744us/step - loss: 0.2994 - accuracy: 0.8574\n",
      "Epoch 21/100\n",
      "586/586 [==============================] - 0s 732us/step - loss: 0.2982 - accuracy: 0.8581\n",
      "Epoch 22/100\n",
      "586/586 [==============================] - 0s 754us/step - loss: 0.2978 - accuracy: 0.8576\n",
      "Epoch 23/100\n",
      "586/586 [==============================] - 0s 779us/step - loss: 0.2967 - accuracy: 0.8594\n",
      "Epoch 24/100\n",
      "586/586 [==============================] - 0s 740us/step - loss: 0.2961 - accuracy: 0.8594\n",
      "Epoch 25/100\n",
      "586/586 [==============================] - 0s 745us/step - loss: 0.2951 - accuracy: 0.8611\n",
      "Epoch 26/100\n",
      "586/586 [==============================] - 0s 743us/step - loss: 0.2948 - accuracy: 0.8614\n",
      "Epoch 27/100\n",
      "586/586 [==============================] - 0s 768us/step - loss: 0.2936 - accuracy: 0.8622\n",
      "Epoch 28/100\n",
      "586/586 [==============================] - 0s 778us/step - loss: 0.2929 - accuracy: 0.8616\n",
      "Epoch 29/100\n",
      "586/586 [==============================] - 0s 758us/step - loss: 0.2919 - accuracy: 0.8620\n",
      "Epoch 30/100\n",
      "586/586 [==============================] - 0s 781us/step - loss: 0.2912 - accuracy: 0.8622\n",
      "Epoch 31/100\n",
      "586/586 [==============================] - 0s 737us/step - loss: 0.2907 - accuracy: 0.8634\n",
      "Epoch 32/100\n",
      "586/586 [==============================] - 0s 776us/step - loss: 0.2902 - accuracy: 0.8633\n",
      "Epoch 33/100\n",
      "586/586 [==============================] - 0s 776us/step - loss: 0.2894 - accuracy: 0.8638\n",
      "Epoch 34/100\n",
      "586/586 [==============================] - 0s 731us/step - loss: 0.2889 - accuracy: 0.8641\n",
      "Epoch 35/100\n",
      "586/586 [==============================] - 0s 849us/step - loss: 0.2882 - accuracy: 0.8639\n",
      "Epoch 36/100\n",
      "586/586 [==============================] - 0s 757us/step - loss: 0.2874 - accuracy: 0.8650\n",
      "Epoch 37/100\n",
      "586/586 [==============================] - 0s 745us/step - loss: 0.2863 - accuracy: 0.8645\n",
      "Epoch 38/100\n",
      "586/586 [==============================] - 0s 747us/step - loss: 0.2855 - accuracy: 0.8641\n",
      "Epoch 39/100\n",
      "586/586 [==============================] - 0s 752us/step - loss: 0.2859 - accuracy: 0.8649\n",
      "Epoch 40/100\n",
      "586/586 [==============================] - 0s 743us/step - loss: 0.2846 - accuracy: 0.8644\n",
      "Epoch 41/100\n",
      "586/586 [==============================] - 0s 736us/step - loss: 0.2841 - accuracy: 0.8660\n",
      "Epoch 42/100\n",
      "586/586 [==============================] - 0s 752us/step - loss: 0.2840 - accuracy: 0.8682\n",
      "Epoch 43/100\n",
      "586/586 [==============================] - 0s 744us/step - loss: 0.2832 - accuracy: 0.8657\n",
      "Epoch 44/100\n",
      "586/586 [==============================] - 0s 737us/step - loss: 0.2830 - accuracy: 0.8665\n",
      "Epoch 45/100\n",
      "586/586 [==============================] - 0s 792us/step - loss: 0.2824 - accuracy: 0.8661\n",
      "Epoch 46/100\n",
      "586/586 [==============================] - 0s 774us/step - loss: 0.2814 - accuracy: 0.8671\n",
      "Epoch 47/100\n",
      "586/586 [==============================] - 0s 741us/step - loss: 0.2814 - accuracy: 0.8671\n",
      "Epoch 48/100\n",
      "586/586 [==============================] - 0s 751us/step - loss: 0.2813 - accuracy: 0.8677\n",
      "Epoch 49/100\n",
      "586/586 [==============================] - 0s 736us/step - loss: 0.2804 - accuracy: 0.8687\n",
      "Epoch 50/100\n",
      "586/586 [==============================] - 0s 766us/step - loss: 0.2801 - accuracy: 0.8684\n",
      "Epoch 51/100\n",
      "586/586 [==============================] - 0s 769us/step - loss: 0.2800 - accuracy: 0.8680\n",
      "Epoch 52/100\n",
      "586/586 [==============================] - 0s 844us/step - loss: 0.2793 - accuracy: 0.8674\n",
      "Epoch 53/100\n",
      "586/586 [==============================] - 0s 764us/step - loss: 0.2794 - accuracy: 0.8681\n",
      "Epoch 54/100\n",
      "586/586 [==============================] - 0s 771us/step - loss: 0.2786 - accuracy: 0.8676\n",
      "Epoch 55/100\n",
      "586/586 [==============================] - 0s 745us/step - loss: 0.2784 - accuracy: 0.8681\n",
      "Epoch 56/100\n",
      "586/586 [==============================] - 0s 776us/step - loss: 0.2784 - accuracy: 0.8680\n",
      "Epoch 57/100\n",
      "586/586 [==============================] - 0s 775us/step - loss: 0.2777 - accuracy: 0.8686\n",
      "Epoch 58/100\n",
      "586/586 [==============================] - 0s 825us/step - loss: 0.2776 - accuracy: 0.8688\n",
      "Epoch 59/100\n",
      "586/586 [==============================] - 0s 774us/step - loss: 0.2776 - accuracy: 0.8676\n",
      "Epoch 60/100\n",
      "586/586 [==============================] - 0s 774us/step - loss: 0.2772 - accuracy: 0.8695\n",
      "Epoch 61/100\n",
      "586/586 [==============================] - 0s 769us/step - loss: 0.2773 - accuracy: 0.8684\n",
      "Epoch 62/100\n",
      "586/586 [==============================] - 0s 759us/step - loss: 0.2768 - accuracy: 0.8690\n",
      "Epoch 63/100\n",
      "586/586 [==============================] - 0s 810us/step - loss: 0.2762 - accuracy: 0.8708\n",
      "Epoch 64/100\n",
      "586/586 [==============================] - 0s 836us/step - loss: 0.2764 - accuracy: 0.8687\n",
      "Epoch 65/100\n",
      "586/586 [==============================] - 0s 848us/step - loss: 0.2755 - accuracy: 0.8678\n",
      "Epoch 66/100\n",
      "586/586 [==============================] - 1s 884us/step - loss: 0.2761 - accuracy: 0.8700\n",
      "Epoch 67/100\n",
      "586/586 [==============================] - 0s 805us/step - loss: 0.2753 - accuracy: 0.8696\n",
      "Epoch 68/100\n",
      "586/586 [==============================] - 0s 783us/step - loss: 0.2754 - accuracy: 0.8699\n",
      "Epoch 69/100\n",
      "586/586 [==============================] - 1s 882us/step - loss: 0.2750 - accuracy: 0.8685\n",
      "Epoch 70/100\n",
      "586/586 [==============================] - 0s 820us/step - loss: 0.2746 - accuracy: 0.8695\n",
      "Epoch 71/100\n",
      "586/586 [==============================] - 0s 794us/step - loss: 0.2750 - accuracy: 0.8692\n",
      "Epoch 72/100\n",
      "586/586 [==============================] - 0s 832us/step - loss: 0.2743 - accuracy: 0.8702\n",
      "Epoch 73/100\n",
      "586/586 [==============================] - 0s 807us/step - loss: 0.2744 - accuracy: 0.8692\n",
      "Epoch 74/100\n",
      "586/586 [==============================] - 0s 770us/step - loss: 0.2745 - accuracy: 0.8696\n",
      "Epoch 75/100\n",
      "586/586 [==============================] - 0s 766us/step - loss: 0.2737 - accuracy: 0.8708\n",
      "Epoch 76/100\n",
      "586/586 [==============================] - 0s 824us/step - loss: 0.2735 - accuracy: 0.8695\n",
      "Epoch 77/100\n",
      "586/586 [==============================] - 0s 780us/step - loss: 0.2733 - accuracy: 0.8699\n",
      "Epoch 78/100\n",
      "586/586 [==============================] - 0s 773us/step - loss: 0.2731 - accuracy: 0.8696\n",
      "Epoch 79/100\n",
      "586/586 [==============================] - 0s 763us/step - loss: 0.2730 - accuracy: 0.8701\n",
      "Epoch 80/100\n",
      "586/586 [==============================] - 0s 774us/step - loss: 0.2730 - accuracy: 0.8699\n",
      "Epoch 81/100\n",
      "586/586 [==============================] - 0s 791us/step - loss: 0.2725 - accuracy: 0.8706\n",
      "Epoch 82/100\n",
      "586/586 [==============================] - 0s 776us/step - loss: 0.2728 - accuracy: 0.8710\n",
      "Epoch 83/100\n",
      "586/586 [==============================] - 0s 782us/step - loss: 0.2720 - accuracy: 0.8717\n",
      "Epoch 84/100\n",
      "586/586 [==============================] - 0s 793us/step - loss: 0.2723 - accuracy: 0.8709\n",
      "Epoch 85/100\n",
      "586/586 [==============================] - 0s 800us/step - loss: 0.2719 - accuracy: 0.8709\n",
      "Epoch 86/100\n",
      "586/586 [==============================] - 0s 789us/step - loss: 0.2718 - accuracy: 0.8710\n",
      "Epoch 87/100\n",
      "586/586 [==============================] - 0s 781us/step - loss: 0.2717 - accuracy: 0.8713\n",
      "Epoch 88/100\n",
      "586/586 [==============================] - 0s 784us/step - loss: 0.2717 - accuracy: 0.8719\n",
      "Epoch 89/100\n",
      "586/586 [==============================] - 0s 791us/step - loss: 0.2718 - accuracy: 0.8719\n",
      "Epoch 90/100\n",
      "586/586 [==============================] - 0s 816us/step - loss: 0.2710 - accuracy: 0.8706\n",
      "Epoch 91/100\n",
      "586/586 [==============================] - 1s 1ms/step - loss: 0.2708 - accuracy: 0.8717\n",
      "Epoch 92/100\n",
      "586/586 [==============================] - 1s 1ms/step - loss: 0.2707 - accuracy: 0.8712\n",
      "Epoch 93/100\n",
      "586/586 [==============================] - 1s 1ms/step - loss: 0.2708 - accuracy: 0.8718\n",
      "Epoch 94/100\n",
      "586/586 [==============================] - 1s 892us/step - loss: 0.2706 - accuracy: 0.8717\n",
      "Epoch 95/100\n",
      "586/586 [==============================] - 0s 837us/step - loss: 0.2705 - accuracy: 0.8708\n",
      "Epoch 96/100\n",
      "586/586 [==============================] - 1s 1ms/step - loss: 0.2702 - accuracy: 0.8721\n",
      "Epoch 97/100\n",
      "586/586 [==============================] - 1s 967us/step - loss: 0.2700 - accuracy: 0.8716\n",
      "Epoch 98/100\n",
      "586/586 [==============================] - 0s 788us/step - loss: 0.2697 - accuracy: 0.8731\n",
      "Epoch 99/100\n",
      "586/586 [==============================] - 1s 866us/step - loss: 0.2697 - accuracy: 0.8716\n",
      "Epoch 100/100\n",
      "586/586 [==============================] - 1s 858us/step - loss: 0.2695 - accuracy: 0.8723\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 - 0s - loss: 0.4683 - accuracy: 0.8120 - 120ms/epoch - 614us/step\n",
      "Loss: 0.46826446056365967, Accuracy: 0.8119999766349792\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
